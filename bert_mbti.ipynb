{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iUB4juIWwrW9iIgC504QUb5tqVRvmEMj",
      "authorship_tag": "ABX9TyNLrPcnH/bZkVn5QWRxIvU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeroxy/colab_pub/blob/main/bert_mbti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/zeyadkhalid/mbti-personality-types-500-dataset\n",
        "\n",
        "데이터셋 부터 먼저 다운로드하여 준비 해 둡시다."
      ],
      "metadata": {
        "id": "PKSAn5TcKvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastparquet transformers[torch]"
      ],
      "metadata": {
        "id": "akaHfQvuOc-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYJfMkxUAybZ",
        "outputId": "412196a0-fec1-42c6-aef6-fb86e032e888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 106067 entries, 0 to 106066\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   posts   106067 non-null  object\n",
            " 1   type    106067 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "================================\n",
            "                                                    posts    type\n",
            "count                                              106067  106067\n",
            "unique                                             106067      16\n",
            "top     know intj tool use interaction people excuse a...    INTP\n",
            "freq                                                    1   24961\n",
            "================================\n",
            "type    INTP   INTJ   INFJ   INFP   ENTP  ENFP  ISTP  ENTJ  ESTP  ENFJ  ISTJ  \\\n",
            "posts  24961  22427  14963  12134  11725  6167  3424  2955  1986  1534  1243   \n",
            "\n",
            "type   ISFP  ISFJ  ESTJ  ESFP  ESFJ  \n",
            "posts   875   650   482   360   181  \n",
            "================================\n",
            "I : 80677, E : 25390, \n",
            "N : 96866, S : 9201, \n",
            "T : 69203, F : 36864, \n",
            "P : 61632, J : 44435, \n",
            " 0 : ENFJ\n",
            " 1 : ENFP\n",
            " 2 : ENTJ\n",
            " 3 : ENTP\n",
            " 4 : ESFJ\n",
            " 5 : ESFP\n",
            " 6 : ESTJ\n",
            " 7 : ESTP\n",
            " 8 : INFJ\n",
            " 9 : INFP\n",
            "10 : INTJ\n",
            "11 : INTP\n",
            "12 : ISFJ\n",
            "13 : ISFP\n",
            "14 : ISTJ\n",
            "15 : ISTP\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_count_by_type(data, character = 'I'):\n",
        "    return data[data['type'].str.contains(character)]['type'].count()\n",
        "\n",
        "train_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_train.parquet'\n",
        "test_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_test.parquet'\n",
        "\n",
        "\n",
        "# df = pd.read_csv('sample_data/MBTI 500.csv')\n",
        "df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/mbti_data/MBTI 500.parquet', engine='fastparquet')\n",
        "\n",
        "pprint( df.info() ) # 무슨 칼럼이 있는지, 각 칼럼별 데이터는 얼마나 있는지 확인\n",
        "print(\"====\"*8)\n",
        "\n",
        "pprint( df.describe() ) # 데이터의 추가 정보들, unique 값의 갯수 등을 출력\n",
        "print(\"====\"*8)\n",
        "\n",
        "# MBTI 종류별로 데이터 분포가 어떻게 되는지 확인\n",
        "pprint( df.groupby(\"type\").count().sort_values('posts', ascending=False).T )\n",
        "print(\"====\"*8)\n",
        "\n",
        "for a, b in [('I','E'), ('N','S'), ('T','F'), ('P','J')]:\n",
        "    print(f'{a} : {get_count_by_type(df, a)}, {b} : {get_count_by_type(df, b)}, ')\n",
        "\n",
        "# MBTI 종류를 numeric 으로 매칭 시키고, id_to_label 생성\n",
        "df['type_cd'] , origin_labels = pd.factorize( df['type'], sort=True)[:2]\n",
        "id_to_label = [x for x in origin_labels]\n",
        "#pprint(id_to_label)\n",
        "for i , x in enumerate(id_to_label):\n",
        "    print(f'{i:2} : {x}')\n",
        "print(\"====\"*8)\n",
        "\n",
        "### 학습용 과 테스트용 데이터를 92: 8 비율로 나눔.\n",
        "train, test = train_test_split(df, test_size = 0.08)\n",
        "\n",
        "### csv 보다 압축률이 좋은 parquet 형태로 저장해 뒀다가 사용 예정\n",
        "train.to_parquet(train_data_path, engine='fastparquet', compression='gzip')\n",
        "test.to_parquet(test_data_path, engine='fastparquet', compression='gzip')\n",
        "print(\"====\"*8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "train_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_train.parquet'\n",
        "test_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_test.parquet'\n",
        "\n",
        "id_to_label = ['ENFJ',  'ENFP',  'ENTJ',  'ENTP',  'ESFJ',  'ESFP',  'ESTJ',  'ESTP',\n",
        "            'INFJ',  'INFP',  'INTJ',  'INTP',  'ISFJ',  'ISFP',  'ISTJ',  'ISTP']\n",
        "\n",
        "def compute_metrics(pred): #모델 정확도를 확인하기 위한 metric 생성 함\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions\n",
        "    acc = top_k_accuracy_score(labels, preds, k=1)\n",
        "    return {'accuracy':acc,} #dict 형태로 여러 지표 추가 가능\n",
        "\n",
        "def get_test_func(model, tokenizer):\n",
        "    def test_mbti(target_text, model=model, tokenizer=tokenizer):\n",
        "        print(target_text)\n",
        "        print(tokenizer.tokenize(target_text))\n",
        "        sample_test_input = get_token_by_tokenizer(tokenizer, target_text)\n",
        "        model.eval()\n",
        "        result = model( **sample_test_input)['logits'].detach().to('cpu')\n",
        "        result_idx = result.argmax(-1).item()\n",
        "        print(result, result_idx)\n",
        "        print(id_to_label[result_idx])\n",
        "        model.train()\n",
        "    return test_mbti\n",
        "\n",
        "def get_token_by_tokenizer(tokenizer, target_text):\n",
        "    return tokenizer.encode_plus(\n",
        "                            target_text,\n",
        "                            add_special_tokens = True,     # '[CLS]', '[SEP]' 같은 토큰 추가\n",
        "                            max_length = 512,              # 최대 token 길이\n",
        "                            padding = 'max_length',        # 최대 길이 만큼 길이 맞춤\n",
        "                            truncation = True,             # 최대 길이 이상의 토큰일시 길이에 맞춰 자름\n",
        "                            return_attention_mask = True,\n",
        "                            return_tensors = 'pt',\n",
        "    )\n",
        "\n",
        "class MBTIDataset(Dataset):\n",
        "    def __init__(self, parquet_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = pd.read_parquet(parquet_path, engine='fastparquet') # parquet 파일 path를 받아서 dataset 객체 생성\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        result = self.df.iloc[idx]\n",
        "        tempresult = get_token_by_tokenizer(self.tokenizer, result['posts'])\n",
        "        return {'input_ids' : tempresult['input_ids'][0],\n",
        "                'attention_mask' : tempresult['attention_mask'][0],\n",
        "                'labels' : result['type_cd']\n",
        "                }\n",
        "\n",
        "bert_name = 'bert-base-uncased' # 사용 할 기본 모델의 위치\n",
        "class_nums = 16 # MBTI 는 총 16종류\n",
        "model     = BertForSequenceClassification.from_pretrained(bert_name, num_labels = class_nums)\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "\n",
        "test_function = get_test_func(model, tokenizer) # 테스트용 함수를 반환 받아 둠\n",
        "\n",
        "train_dataset = MBTIDataset(train_data_path, tokenizer)\n",
        "test_dataset = MBTIDataset(test_data_path, tokenizer)\n",
        "print(f'train data rows : {len(train_dataset):8}')\n",
        "print(f'test data rows  : {len(test_dataset):8}')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'mbti_test',\n",
        "    num_train_epochs = 8.0,            # 8.0 번 반복 학습\n",
        "    evaluation_strategy = 'epoch',     # 매 epoch 마다 모델 정확도 측정\n",
        "    per_device_train_batch_size = 32,  # 한 배치 입력에 32개 데이터 입력 ( train 용 )\n",
        "    per_device_eval_batch_size  = 32,  # 한 배치 입력에 32개 데이터 입력 ( evaluation 용)\n",
        "    gradient_accumulation_steps = 8,   # 배치 8개를 묶어서 학습량 계산.\n",
        "    gradient_checkpointing = True,     # gradient_checkpointing 을 활성화 하여 계산량이 조금 늘지만, gpu 메모리를 절약\n",
        "    fp16 = True,\n",
        "    save_strategy = 'epoch',\n",
        "    save_total_limit = 3,\n",
        "    dataloader_num_workers = 4         # data loader 를 4개 워커를 이용해 수행.\n",
        ")\n",
        "\n",
        "########################################\n",
        "\n",
        "print('학습 전')\n",
        "target_text = \"I'm ironman\"\n",
        "test_function(target_text)\n",
        "\n",
        "########################################\n",
        "for n, p in model.named_parameters():\n",
        "    if 'bert' in n:\n",
        "        p.require_grad = False\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model('mbti_ckpt_train_last_layer')\n",
        "\n",
        "########################################\n",
        "\n",
        "print('마지막 레이어만 학습 후')\n",
        "test_function(target_text)\n",
        "\n",
        "########################################"
      ],
      "metadata": {
        "id": "QeswlvzNE903"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTFoE9r4VdnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}