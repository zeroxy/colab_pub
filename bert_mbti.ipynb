{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iUB4juIWwrW9iIgC504QUb5tqVRvmEMj",
      "authorship_tag": "ABX9TyMcH2E5zFr7vjELJR+71cJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeroxy/colab_pub/blob/main/bert_mbti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastparquet transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akaHfQvuOc-6",
        "outputId": "445c503a-fafc-4180-ca64-957800b2482f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2023.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[torch]\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.23.5)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, cramjam, huggingface-hub, transformers, fastparquet, accelerate\n",
            "Successfully installed accelerate-0.22.0 cramjam-2.7.0 fastparquet-2023.7.0 huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYJfMkxUAybZ",
        "outputId": "412196a0-fec1-42c6-aef6-fb86e032e888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 106067 entries, 0 to 106066\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   posts   106067 non-null  object\n",
            " 1   type    106067 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "================================\n",
            "                                                    posts    type\n",
            "count                                              106067  106067\n",
            "unique                                             106067      16\n",
            "top     know intj tool use interaction people excuse a...    INTP\n",
            "freq                                                    1   24961\n",
            "================================\n",
            "type    INTP   INTJ   INFJ   INFP   ENTP  ENFP  ISTP  ENTJ  ESTP  ENFJ  ISTJ  \\\n",
            "posts  24961  22427  14963  12134  11725  6167  3424  2955  1986  1534  1243   \n",
            "\n",
            "type   ISFP  ISFJ  ESTJ  ESFP  ESFJ  \n",
            "posts   875   650   482   360   181  \n",
            "================================\n",
            "I : 80677, E : 25390, \n",
            "N : 96866, S : 9201, \n",
            "T : 69203, F : 36864, \n",
            "P : 61632, J : 44435, \n",
            " 0 : ENFJ\n",
            " 1 : ENFP\n",
            " 2 : ENTJ\n",
            " 3 : ENTP\n",
            " 4 : ESFJ\n",
            " 5 : ESFP\n",
            " 6 : ESTJ\n",
            " 7 : ESTP\n",
            " 8 : INFJ\n",
            " 9 : INFP\n",
            "10 : INTJ\n",
            "11 : INTP\n",
            "12 : ISFJ\n",
            "13 : ISFP\n",
            "14 : ISTJ\n",
            "15 : ISTP\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_count_by_type(data, character = 'I'):\n",
        "    return data[data['type'].str.contains(character)]['type'].count()\n",
        "\n",
        "train_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_train.parquet'\n",
        "test_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_test.parquet'\n",
        "\n",
        "\n",
        "# df = pd.read_csv('sample_data/MBTI 500.csv')\n",
        "df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/mbti_data/MBTI 500.parquet', engine='fastparquet')\n",
        "\n",
        "pprint( df.info() ) # 무슨 칼럼이 있는지, 각 칼럼별 데이터는 얼마나 있는지 확인\n",
        "print(\"====\"*8)\n",
        "\n",
        "pprint( df.describe() ) # 데이터의 추가 정보들, unique 값의 갯수 등을 출력\n",
        "print(\"====\"*8)\n",
        "\n",
        "# MBTI 종류별로 데이터 분포가 어떻게 되는지 확인\n",
        "pprint( df.groupby(\"type\").count().sort_values('posts', ascending=False).T )\n",
        "print(\"====\"*8)\n",
        "\n",
        "for a, b in [('I','E'), ('N','S'), ('T','F'), ('P','J')]:\n",
        "    print(f'{a} : {get_count_by_type(df, a)}, {b} : {get_count_by_type(df, b)}, ')\n",
        "\n",
        "# MBTI 종류를 numeric 으로 매칭 시키고, id_to_label 생성\n",
        "df['type_cd'] , origin_labels = pd.factorize( df['type'], sort=True)[:2]\n",
        "id_to_label = [x for x in origin_labels]\n",
        "#pprint(id_to_label)\n",
        "for i , x in enumerate(id_to_label):\n",
        "    print(f'{i:2} : {x}')\n",
        "print(\"====\"*8)\n",
        "\n",
        "### 학습용 과 테스트용 데이터를 92: 8 비율로 나눔.\n",
        "train, test = train_test_split(df, test_size = 0.08)\n",
        "\n",
        "### csv 보다 압축률이 좋은 parquet 형태로 저장해 뒀다가 사용 예정\n",
        "train.to_parquet(train_data_path, engine='fastparquet', compression='gzip')\n",
        "test.to_parquet(test_data_path, engine='fastparquet', compression='gzip')\n",
        "print(\"====\"*8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "train_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_train.parquet'\n",
        "test_data_path = 'drive/MyDrive/Colab Notebooks/mbti_data/MBTI_500_test.parquet'\n",
        "\n",
        "id_to_label = ['ENFJ',  'ENFP',  'ENTJ',  'ENTP',  'ESFJ',  'ESFP',  'ESTJ',  'ESTP',\n",
        "            'INFJ',  'INFP',  'INTJ',  'INTP',  'ISFJ',  'ISFP',  'ISTJ',  'ISTP']\n",
        "\n",
        "def compute_metrics(pred): #모델 정확도를 확인하기 위한 metric 생성 함\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions\n",
        "    acc = top_k_accuracy_score(labels, preds, k=1)\n",
        "    return {'accuracy':acc,} #dict 형태로 여러 지표 추가 가능\n",
        "\n",
        "def get_test_func(model, tokenizer):\n",
        "    def test_mbti(target_text, model=model, tokenizer=tokenizer):\n",
        "        print(target_text)\n",
        "        print(tokenizer.tokenize(target_text))\n",
        "        sample_test_input = get_token_by_tokenizer(tokenizer, target_text)\n",
        "        model.eval()\n",
        "        result = model( **sample_test_input)['logits'].detach().to('cpu')\n",
        "        result_idx = result.argmax(-1).item()\n",
        "        print(result, result_idx)\n",
        "        print(id_to_label[result_idx])\n",
        "        model.train()\n",
        "    return test_mbti\n",
        "\n",
        "def get_token_by_tokenizer(tokenizer, target_text):\n",
        "    return tokenizer.encode_plus(\n",
        "                            target_text,\n",
        "                            add_special_tokens = True,     # '[CLS]', '[SEP]' 같은 토큰 추가\n",
        "                            max_length = 512,              # 최대 token 길이\n",
        "                            padding = 'max_length',        # 최대 길이 만큼 길이 맞춤\n",
        "                            truncation = True,             # 최대 길이 이상의 토큰일시 길이에 맞춰 자름\n",
        "                            return_attention_mask = True,\n",
        "                            return_tensors = 'pt',\n",
        "    )\n",
        "\n",
        "class MBTIDataset(Dataset):\n",
        "    def __init__(self, parquet_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = pd.read_parquet(parquet_path, engine='fastparquet') # parquet 파일 path를 받아서 dataset 객체 생성\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        result = self.df.iloc[idx]\n",
        "        tempresult = get_token_by_tokenizer(self.tokenizer, result['posts'])\n",
        "        return {'input_ids' : tempresult['input_ids'][0],\n",
        "                'attention_mask' : tempresult['attention_mask'][0],\n",
        "                'labels' : result['type_cd']\n",
        "                }\n",
        "\n",
        "bert_name = 'bert-base-uncased' # 사용 할 기본 모델의 위치\n",
        "class_nums = 16 # MBTI 는 총 16종류\n",
        "model     = BertForSequenceClassification.from_pretrained(bert_name, num_labels = class_nums)\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "\n",
        "test_function = get_test_func(model, tokenizer) # 테스트용 함수를 반환 받아 둠\n",
        "\n",
        "train_dataset = MBTIDataset(train_data_path, tokenizer)\n",
        "test_dataset = MBTIDataset(test_data_path, tokenizer)\n",
        "print(f'train data rows : {len(train_dataset):8}')\n",
        "print(f'test data rows  : {len(test_dataset):8}')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'mbti_test',\n",
        "    num_train_epochs = 8.0,            # 8.0 번 반복 학습\n",
        "    evaluation_strategy = 'epoch',     # 매 epoch 마다 모델 정확도 측정\n",
        "    per_device_train_batch_size = 32,  # 한 배치 입력에 32개 데이터 입력 ( train 용 )\n",
        "    per_device_eval_batch_size  = 32,  # 한 배치 입력에 32개 데이터 입력 ( evaluation 용)\n",
        "    gradient_accumulation_steps = 8,   # 배치 8개를 묶어서 학습량 계산.\n",
        "    gradient_checkpointing = True,     # gradient_checkpointing 을 활성화 하여 계산량이 조금 늘지만, gpu 메모리를 절약\n",
        "    fp16 = True,\n",
        "    save_strategy = 'epoch',\n",
        "    save_total_limit = 3,\n",
        "    dataloader_num_workers = 4         # data loader 를 4개 워커를 이용해 수행.\n",
        ")\n",
        "\n",
        "########################################\n",
        "\n",
        "print('학습 전')\n",
        "target_text = \"I'm ironman\"\n",
        "test_function(target_text)\n",
        "\n",
        "########################################\n",
        "for n, p in model.named_parameters():\n",
        "    if 'bert' in n:\n",
        "        p.require_grad = False\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model('mbti_ckpt_train_last_layer')\n",
        "\n",
        "########################################\n",
        "\n",
        "print('마지막 레이어만 학습 후')\n",
        "test_function(target_text)\n",
        "\n",
        "########################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "QeswlvzNE903",
        "outputId": "ed71b738-386f-426e-8594-ce95872614ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data rows :    97581\n",
            "test data rows  :     8486\n",
            "학습 전\n",
            "I'm ironman\n",
            "['i', \"'\", 'm', 'iron', '##man']\n",
            "tensor([[ 0.1656, -0.1471,  0.3543,  0.4787, -0.0295, -0.1781,  0.3643, -0.4205,\n",
            "         -0.1315,  0.0849,  0.2964,  0.3391, -0.2100, -0.3573, -0.2464, -0.0404]]) 3\n",
            "ENTP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='3048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  68/3048 12:24 < 9:19:58, 0.09 it/s, Epoch 0.18/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTFoE9r4VdnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}